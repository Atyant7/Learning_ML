{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb60ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7142857142857143\n",
      "Precision:  0.75\n",
      "Recall:  0.75\n",
      "F1 Score:  0.75\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "-> model accuracy\n",
    "-> kitna error hai / how far guesses were\n",
    "-> how often its wrong or right - precision\n",
    "-> use only for scoring the results\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score\n",
    "\n",
    "#True answers (what actually happened)\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0]\n",
    "\n",
    "# Model prediction (what it guessed)\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 1]\n",
    "\n",
    "#evaluation\n",
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision: \", precision_score(y_true, y_pred))\n",
    "print(\"Recall: \", recall_score(y_true, y_pred))\n",
    "print(\"F1 Score: \", f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c833b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[4 1]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "# confusion Metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
    "\n",
    "cm = confusion_matrix(y_true , y_pred)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc67b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: On average off by: 7.5\n",
      "MSE: Squared mistake value: 62.5\n",
      "RMSE: Final Realistic Error: 7.905694150420948\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "MAE - Mean Absolute Error\n",
    "1- take the mistake difference\n",
    "2- remove the minus sign\n",
    "3- add all the values\n",
    "4- divide by total no. of prediction\n",
    "\n",
    "MSE - Mean Squared Error\n",
    "1- making the error more big by squaring it\n",
    "2- add all the values\n",
    "3- then divide by the total prediction (simple and not squared)\n",
    "\n",
    "RMSE - Root Mean Squared Error\n",
    "1- Root of the MSE\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error , mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "#real scores\n",
    "real_scores = [90, 60, 80, 100]\n",
    "\n",
    "#model guess\n",
    "pred_scores = [85, 70, 70, 95]\n",
    "\n",
    "mae = mean_absolute_error(real_scores , pred_scores)\n",
    "\n",
    "mse = mean_squared_error(real_scores , pred_scores)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"MAE: On average off by:\",mae)\n",
    "print(\"MSE: Squared mistake value:\",mse)\n",
    "print(\"RMSE: Final Realistic Error:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14db97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
